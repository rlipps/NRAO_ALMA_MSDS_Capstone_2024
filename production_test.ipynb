{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "import string\n",
    "from joblib import load\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# NRAO Stopword list\n",
    "nrao_stops = ['0','1','2','3','4','5','6','7','8','9',\"=\",'<','>','~',\"/\",'`',\".\",\",\", 'observation'\n",
    "                       'alma','resolution','source','show','sample','high','use','observe'\n",
    "                       'figure','low','image','propose', 'also','use', 'large', 'study'\n",
    "                       'reference', 'detect','see','well', 'time', 'however', 'expect'\n",
    "                       'provide','datum', 'model', 'result','sensitivity','scale'\n",
    "                       'find','allow','scientific','target','compare','resolve','first',\n",
    "                       'leave', 'estimate','suggest', 'due', 'obtain', 'small', 'measure',\n",
    "                       'include','property','justification', 'right ', 'understand', 'similar',\n",
    "                       'detection', 'require', 'indicate', 'order', 'range', 'make','map','thus',\n",
    "                       'follow','fig','goal','proposal','field','determine','therefore', 'reveal',\n",
    "                       'give', 'process','total', 'important', 'know','constrain', 'ratio','even',\n",
    "                       'case','et', 'al', 'pc','kpc','apj','km','mm','m','one','two', 'data', 'us',\n",
    "                       'mnras', 'left', 'right', 'may', 'within','would','need','request','mjy',\n",
    "                       'different','assume','recent','good','since','still','previous','science',\n",
    "                       'ghz','could','object','much','survey','three','whether','likely','several',\n",
    "                       'like','able','identify','new','best','number','analysis','confirm','predict',\n",
    "                       'le','evidence','select','example','take','recently','combine','exist','value',\n",
    "                       'fit','objective','comparison','investigate','respectively','many','although',\n",
    "                       'achieve','cm','jy','need','enough','search','yr','explain','au','apjl','per','arxiv',\n",
    "                       'a&a', 'aa', 'apj', 'apjl', 'mnras', 'pasp', 'aj', 'cycle', 'band',\n",
    "                       'emission', 'free', 'anticipate', 'originate', 'success', 'separate', 'uv', 'significance',\n",
    "                       'hot', 'frequency', 'wavelength', 'realistic', 'mas', 'mg', 'minute', 'ii', 'ad', 'hd',\n",
    "                       'occurrence', 'event', 'myr', 'ra', 'dec',  'ly', 'tau', 'cn',\n",
    "                       'arc', 'ori', 'hh', 'iii', 'cha', 'ab', 'tw', 'ms', 'ngc', 'pds',\n",
    "                       'jwst','hcn','hco+','oh','xray','aca', 'vla', 'gbt', 'proto', 'noema',\n",
    "                       'quiescent','nir', 'heating', 'sb','temperature','cr','hya','liu','warm',\n",
    "                       'nh','extent','spitzer', 'co','yang']\n",
    "\n",
    "# Text processing functions\n",
    "#convert to lowercase, strip and remove punctuations and remove ALMA, case insensitive\n",
    "def preprocess(text):\n",
    "    text = text.lower()                                                         # Make everything lower case\n",
    "    text = text.strip()                                                         # Strip leading and trailing whitespace\n",
    "    text = re.compile('<.*?>').sub('', text)                                    # Remove things like html tags \n",
    "    text = re.compile('[%s]' % re.escape(string.punctuation)).sub(' ', text)    # Remove punctuation\n",
    "    text = re.sub(r'(?i)alma', '', text)                                        # Remove case insensitive 'alma'\n",
    "    text = re.sub('\\s+', ' ', text)                                             # Convert whitespace to single space\n",
    "    text = re.sub(r'\\[[0-9]*\\]',' ',text)                                       # Remove things like citations e.g. [9]\n",
    "    text = re.sub(r'[^\\w\\s]', '', str(text).lower().strip())                    # Remove non alphanumeric characters\n",
    "    text = re.sub(r'\\d',' ',text)                                               # Remove digits\n",
    "    text = re.sub(r'\\s+',' ',text)                                              # Collapse any created whitespace into single space\n",
    "    return text\n",
    " \n",
    "# STOPWORD REMOVAL\n",
    "def stopword(string):\n",
    "    sws = stopwords.words('english')\n",
    "    sws.extend(nrao_stops)\n",
    "    a= [i for i in string.split() if i not in sws]\n",
    "    return ' '.join(a)\n",
    "\n",
    "#LEMMATIZATION\n",
    "# Initialize the lemmatizer\n",
    "wl = WordNetLemmatizer()\n",
    " \n",
    "# This is a helper function to map NTLK position tags\n",
    "def get_wordnet_pos(tag):\n",
    "    if tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "def lemmatizer(string):\n",
    "    word_pos_tags = nltk.pos_tag(word_tokenize(string)) # Get position tags\n",
    "    a=[wl.lemmatize(tag[0], get_wordnet_pos(tag[1])) for idx, tag in enumerate(word_pos_tags)] # Map the position tag and lemmatize the word/token\n",
    "    return \" \".join(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted probability of only continuum measurements: 1.032\n",
      "Predicted probability of at least one line measurement: 98.968\n",
      "Predicted project topic number: 36\n",
      "Top two predicted bands: [4 8]\n"
     ]
    }
   ],
   "source": [
    "# Import models from joblib files\n",
    "logreg_tfidf_vectorizer = load('models/line_continuum_classifier/tfidf_vectorizer_logreg.joblib')\n",
    "logreg_classifier = load('models/line_continuum_classifier/log_model.joblib')\n",
    "naive_bayes_tfidf_vectorizer = load('models/band_classification/tfidf_vectorizer_naive_bayes.joblib')\n",
    "naive_bayes_model = load('models/band_classification/naive_bayes_model.joblib')\n",
    "lda_count_vectorizer = load('models/topic_mining/lda_count_vectorizer.joblib')\n",
    "lda_model = load('models/topic_mining/lda_model.joblib')\n",
    "\n",
    "# Prompt user for title and abstract\n",
    "title = input('Enter project title:')\n",
    "abstract = input('Enter project abstract:')\n",
    "raw_text = title + '. ' + abstract\n",
    "\n",
    "# Create dataframe of input with various text processing required for models\n",
    "df = pd.DataFrame({\n",
    "    'raw_text':raw_text\n",
    "}, index=[1])\n",
    "\n",
    "df['std_text'] = df.raw_text.apply(lambda x: preprocess(x))\n",
    "df['std_text_sw_removed'] = df.std_text.apply(lambda x: stopword(x))\n",
    "df['std_text_sw_removed_lemmatized'] = df.std_text_sw_removed.apply(lambda x: lemmatizer(x))\n",
    "\n",
    "# Predict from models\n",
    "# Logistic Regression for Line/Continuum classification\n",
    "logreg_pred = logreg_classifier.predict_proba(logreg_tfidf_vectorizer.transform(df.std_text))\n",
    "print(f'Predicted probability of only continuum measurements: {round(logreg_pred[0][0]*100, 3)}')\n",
    "print(f'Predicted probability of at least one line measurement: {round(logreg_pred[0][1]*100, 3)}')\n",
    "\n",
    "# # Topic assignment\n",
    "lda_pred = np.argmax(lda_model.transform(lda_count_vectorizer.transform(df.std_text_sw_removed_lemmatized)))\n",
    "print(f'Predicted project topic number: {lda_pred}')\n",
    "\n",
    "# Band prediction\n",
    "band_pred = naive_bayes_model.predict_proba(naive_bayes_tfidf_vectorizer.transform(df.std_text_sw_removed_lemmatized))\n",
    "sorted_indices = np.argsort(band_pred)          # Get indices for all band predictions\n",
    "sorted_indices = np.flip(sorted_indices)        # Sort in descending probability\n",
    "for prediction in range(len(sorted_indices)):   # Manage class relabeling to match AMLA bands\n",
    "    for band in range(len(sorted_indices[prediction])):\n",
    "        if sorted_indices[prediction][band] != 0:\n",
    "            sorted_indices[prediction][band] += 2 # need to add 2 to index to equal the band that was predicted band (3, 4, 5, 6, 7, 8, 9, or 10)\n",
    "        else:\n",
    "            sorted_indices[prediction][band] += 1 # need to add 1 to index to equal the band that was predicted (band 1)\n",
    "print(f'Top two predicted bands: {sorted_indices[0][:2]}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nraotest",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
